<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Yanhua Cheng, Breeze, 程衍华, CRIPAC, NLPR, CASIA, Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, HUST" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="shortcut icon" href="fig/cripac.png">
<title>Yanhua Cheng's Homepage</title>
</head>
<body>
<div id="layout-content">

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<table class="imgtable"><tr><td>
<img src="fig/yhc.jpg" alt="alt text" width="360px" height="314px" /> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
  <h1>
  <a href="http://yanhuacheng.github.io./">Yanhua Cheng</a> &nbsp; 程衍华
  </h1>
</div>

<p>
Ph.D. Candidate, supervised by <a href="http://people.ucas.ac.cn/~huangkaiqi">Prof. Kaiqi Huang</a>
<br />
<br />

Email: <a href="mailto:yanhua.cheng@nlpr.ia.ac.cn">yanhua.cheng at nlpr.ia.ac.cn</a><br />
Address: No.95 ZhongGuanCun East St, HaiDian District, Beijing, P.R.China, 100190<br />
</p>
</td></tr></table>

<h2>News</h2>
<ul>
<li>
  <p>
      Updating soon...
  </p>
</li>
</ul>

<h2>
  Biography 
</h2>
<ul>

<li>
  <p>
    I am a 4th-year M.D-Ph.D. student in <a href="http://www.nlpr.ia.ac.cn/">National Laboratory of Pattern Recognition (NLPR)</a>, <a href="http://english.ia.cas.cn/">Institute of Automation Chinese Academy of Sciences (CASIA)</a>. My supervisor is Prof. <a href="http://people.ucas.ac.cn/~huangkaiqi">Kaiqi Huang</a>. I am a member of <a href="http://www.cripac.ia.ac.cn/">Center for Research on Intelligent Perception and Computing (CRIPAC)</a>.  
  </p>
</li>

<li>
  <p>
    I received the Bachelor degree in <a href="http://auto.hust.edu.cn/">Department of Automation </a> from <a href="http://www.hust.edu.cn">Huazhong University of Science and Techonology (HUST)</a>. 
  </p>
</li>

<li>
  <p>
    My research interests include computer vision,  deep learning, 3D object/scenet understanding.  
  </p>
</li>
</ul>

<h2>Publications</h2> 
<ul>
<li>
<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">Semi-supervised Learning and Feature Evaluation for RGB-D Object Recognition,</a> <br />
, <b>Yanhua Cheng</b>, Xin Zhao, Kaiqi Huang, Tieniu Tan <br />
<i>Computer Vision and Image Understanding</i> (<b>CVIU</b>), 2015, Volume 139, Pages 149-160. <br />
</li>
[<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">PDF</a>]
[<a href="javascript:toggleBibtex('CVIU15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('CVIU15')" target="_self">Bibtex</a>]
[<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">DOI</a>]
<!-- [<a href="https://github.com/cwhgn/EGTracker">Code</a>] -->
<!-- [<a href="http://youtu.be/GZ2u2tvzgi4">Demos</a>] -->
<div class="blockcontent" id="CVIU_abstract" style="display:none"> 
  <p style="font-size:16px">
        With new depth sensing technology such as Kinect providing high quality synchronized
        RGB and depth images (RGB-D data), combining the two distinct views for
        object recognition has attracted great interest in computer vision and robotics community.
        Recent methods mostly employ supervised learning methods for this new RGB-D
        modality based on the two feature sets. However, supervised learning methods always
        depend on large amount of manually labeled data for training models. To address the
        problem, this paper proposes a semi-supervised learning method to reduce the dependence
        on large annotated training sets. The method can effectively learn from relatively
        plentiful unlabeled data, if powerful feature representations for both the RGB
        and depth view can be extracted. Thus, four representative feature extraction methods
        (KDES, CKM, HMP and CNN-RNN) are analysed and evaluated in this paper, and the
        best is selected for the semi-supervised learning. We verify our method on two publicly
        available RGB-D object databases. The experimental results demonstrate that,
        with only 20% labeled training set, the proposed semi-supervised learning method can
        achieve the state-of-the-art performance on the large-scale RGB-D object database, and
        competitive performance on the 2D3D object database.
      </p>
</div>

<div class="blockcontent" id="CVIU15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/CVIU2015,
 title = {Semi-supervised Learning and Feature Evaluation for RGB-D Object Recognition},
 author={Yanhua Cheng, Xin Zhao, Kaiqi Huang, Tieniu Tan},
 booktitle = {CVIU},
 volume={139}, 
 pages={149--160},
 year = {2015}
}
</pre>
</div>
<br /><br />

<li>
<a href="?"> Semi-supervised Multimodal Deep Learning for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>,Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui <br />
<i>International Joint Conference on Artificial Intelligence </i> (<b>IJCAI</b>), 2015 (accepted). <br />
</li>
[<a href="?">PDF</a>]
[<a href="javascript:toggleBibtex('IJCAI2015_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('IJCAI2015')" target="_self">Bibtex</a>]
[<a href="?">DOI</a>]
<div class="blockcontent" id="IJCAI2015_abstract" style="display:none"> 
  <p style="font-size:16px">
                  This paper studies the problem of RGB-D object
        recognition. Inspired by the great success of deep
        convolutional neural networks (DCNN) in AI, researchers
        have tried to apply it to improve the performance
        of RGB-D object recognition. However,
        DCNN always requires a large-scale annotated
        dataset to supervise its training. Manually labeling
        such a large RGB-D dataset is expensive
        and time consuming, which prevents DCNN from
        quickly promoting this research area. To address
        this problem, we propose a semi-supervised multimodal
        deep learning framework to train DCNN
        effectively based on very limited labeled data and
        massive unlabeled data. The core of our framework
        is a novel diversity preserving co-training algorithm,
        which can successfully guide DCNN to
        learn from the unlabeled RGB-D data by making
        full use of the complementary cues of the RGB and
        depth data in object representation. Experiments
        on the benchmark RGB-D dataset demonstrate that,
        with only 5% labeled training data, our approach
        achieves competitive performance for object recognition
        compared with those state-of-the-art results
        reported by fully-supervised methods.
      </p>
</div>

<div class="blockcontent" id="IJCAI2015" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/IJCAI2015,
 title = {Semi-supervised Multimodal Deep Learning for RGB-D Object Recognition},
 author={Yanhua Cheng, Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui},
 booktitle = {IJCAI},
 year = {2016}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1"> Query Adaptive Similarity Measure for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui <br />
<i>International Conference on Computer Vision</i> (<b>ICCV</b>), 2015. <br />
</li>
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">PDF</a>]
[<a href="javascript:toggleBibtex('ICCV15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICCV15')" target="_self">Bibtex</a>]
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">DOI</a>]
<div class="blockcontent" id="ICCV15_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper studies the problem of improving the top-1 accuracy
of RGB-D object recognition. Despite of the impressive
top-5 accuracies achieved by existing methods, their
top-1 accuracies are not very satisfactory. The reasons are
in two-fold: (1) existing similarity measures are sensitive to
object pose and scale changes, as well as intra-class variations;
and (2) effectively fusing RGB and depth cues is still
an open problem. To address these problems, this paper first
proposes a new similarity measure based on dense matching,
through which objects in comparison are warped and
aligned, to better tolerate variations. Towards RGB and
depth fusion, we argue that a constant and golden weight
doesn’t exist. The two modalities have varying contributions
when comparing objects from different categories. To
capture such a dynamic characteristic, a group of matchers
equipped with various fusion weights is constructed, to explore
the responses of dense matching under different fusion
configurations. All the response scores are finally merged
following a learning-to-combination way, which provides
quite good generalization ability in practice. The proposed
approach win the best results on several public benchmarks,
e.g., achieves 92.7% top-1 test accuracy on the Washington
RGB-D object dataset, with a 5.1% improvement over the
state-of-the-art.
      </p>
</div>

<div class="blockcontent" id="ICCV15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/iccv2015,
 title = {Query Adaptive Similarity Measure for RGB-D Object Recognition},
 author={Yanhua Cheng, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui},
 booktitle = {ICCV},
 year = {2015},
 pages = {145-153}
 address = {Chile}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://159.226.251.229/videoplayer/Zhang_MeshStereo_A_Global_ICCV_2015_paper.pdf?ich_u_r_i=8b0c9a1eabbcf68f5daea0e5150e46b5&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663592491&ich_t_y_p_e=1&ich_d_i_s_k_i_d=4&ich_u_n_i_t=1"> MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation, </a> <br />
<b>Yanhua Cheng</b>, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui <br />
<i>International Conference on Computer Vision</i> (<b>ICCV</b>), 2015. <br />
</li>
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">PDF</a>]
[<a href="javascript:toggleBibtex('ICCV15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICCV15')" target="_self">Bibtex</a>]
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">DOI</a>]
<div class="blockcontent" id="ICCV15_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper studies the problem of improving the top-1 accuracy
of RGB-D object recognition. Despite of the impressive
top-5 accuracies achieved by existing methods, their
top-1 accuracies are not very satisfactory. The reasons are
in two-fold: (1) existing similarity measures are sensitive to
object pose and scale changes, as well as intra-class variations;
and (2) effectively fusing RGB and depth cues is still
an open problem. To address these problems, this paper first
proposes a new similarity measure based on dense matching,
through which objects in comparison are warped and
aligned, to better tolerate variations. Towards RGB and
depth fusion, we argue that a constant and golden weight
doesn’t exist. The two modalities have varying contributions
when comparing objects from different categories. To
capture such a dynamic characteristic, a group of matchers
equipped with various fusion weights is constructed, to explore
the responses of dense matching under different fusion
configurations. All the response scores are finally merged
following a learning-to-combination way, which provides
quite good generalization ability in practice. The proposed
approach win the best results on several public benchmarks,
e.g., achieves 92.7% top-1 test accuracy on the Washington
RGB-D object dataset, with a 5.1% improvement over the
state-of-the-art.
      </p>
</div>

<div class="blockcontent" id="ICCV15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/iccv2015,
 title = {Query Adaptive Similarity Measure for RGB-D Object Recognition},
 author={Yanhua Cheng, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui},
 booktitle = {ICCV},
 year = {2015},
 pages = {145-153}
 address = {Chile}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://dl.acm.org/citation.cfm?id=2632875"> License Plate Localization With Efficient Markov Chain Monte Carlo, </a> <br />
Lijun Cao, Xu Zhang, <b>Weihua Chen</b>, Kaiqi Huang <br />
<i>ACM International Conference on Internet Multimedia  Computing and Service</i> (<b>ICIMCS</b>), 2014. <br />
</li>
[<a href="http://delivery.acm.org/10.1145/2640000/2632875/p295-cao.pdf?ip=159.226.179.184&id=2632875&acc=ACTIVE%20SERVICE&key=33E289E220520BFB.949A0B1AADF887FF.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=478662293&CFTOKEN=48147274&__acm__=1423729709_b0f999fdc4df3e522a521c2999917fb0">PDF</a>]
[<a href="javascript:toggleBibtex('ICIMCS14b_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICIMCS14b')" target="_self">Bibtex</a>]
[<a href="http://dl.acm.org/citation.cfm?id=2632875">DOI</a>]
<div class="blockcontent" id="ICIMCS14b_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper presents a novel efficient Markov Chain Monte Carlo (MCMC) method for License Plate (LP) localization. The proposed method formulates the LP image feature and prior knowledge into a unified Bayesian framework. Then the localization problem is derived as a maximizing-a-posterior (MAP) problem, which integrates color, edge and character feature of LP. We propose an efficient MCMC method, taking integrated local geometrical likelihood as proposal probability to make the inference feasible. The experimental results on real dataset are very promising in terms of detection rate and localization accuracy.
      </p>
</div>

<div class="blockcontent" id="ICIMCS14b" style="display:none"> 
<pre>
@inproceedings{Chen/icimcs2014,
 title = {License Plate Localization With Efficient Markov Chain Monte Carlo},
 author={Cao, Lijun and Zhang, Xu and Chen, Weihua and Huang, Kaiqi},
 booktitle = {ACM International Conference on Internet Multimedia  Computing and Service (ICIMCS)},
 year = {2014},
 pages = {295}
 address = {Xiamen, China}
}
</pre>
</div>
<br /><br />
</ul>

<h2>Softwares</h2>
<ul>
<li>
<a href="http://mct.idealtest.org/Evaluation.html">MCTA Evaluation Toolkit </a> : An evaluation kit for computing Multi-Camera Tracking Accuracy (MCTA) in <a href="http://mct.idealtest.org/">the Multi-Camera Object Tracking (<b>MCT</b>) Challenge </a> in <a href="http://eccv2014.org/">ECCV 2014</a> <a href="http://www.vs-re-id-2014.org/">Workshop on Visual Surveillance and Re-identification</a><br /> 
</li>
</ul>

<h2>Datasets</h2>
<ul>
<li>
  <p>
    <a href="http://mct.idealtest.org/Datasets.html">NLPR_MCT Dataset</a> : The dataset consists of four subsets corresponding to different non-overlapping multi-camera networks.
    </p>
</li>
</ul>

<p>
  <a href="http://english.ia.cas.cn/"><img src="fig/casia.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.nlpr.ia.ac.cn/"><img src="fig/nlpr.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.cripac.ia.ac.cn/"><img src="fig/cripac.png" alt="alt text" width="50px" height="50px"/></a> 

  <a href="http://www.bjtu.edu.cn/"><img src="fig/bjtu.jpg" alt="alt text" width="50px" height="50px"/></a> 
</p>

<div id="footer">
<div id="footer-text">
</br>Last updated at 2015-02-12 by Weihua Chen. Page created by reference to <a href="http://www.ee.cuhk.edu.hk/~rzhao/">Rui Zhao's homepage</a>.
</div>
</div>
</div>


</body>
</html>
