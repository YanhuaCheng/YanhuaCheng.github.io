<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Yanhua Cheng, Breeze, 程衍华, CRIPAC, NLPR, CASIA, Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, HUST" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="shortcut icon" href="fig/cripac.png">
<title>Yanhua Cheng's Homepage</title>
</head>
<body>
<div id="layout-content">

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<table class="imgtable"><tr><td>
<img src="fig/yhc.jpg" alt="alt text" width="380px" height="314px" /> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
  <h1>
  <a href="http://yanhuacheng.github.io./">Yanhua Cheng</a> &nbsp; 程衍华
  </h1>
</div>

<p>
Ph.D. Candidate, supervised by <a href="http://people.ucas.ac.cn/~huangkaiqi">Prof. Kaiqi Huang</a>
<br />
<br />

Email: <a href="mailto:yanhua.cheng@nlpr.ia.ac.cn">yanhua.cheng at nlpr.ia.ac.cn</a><br />
Address: No.95 ZhongGuanCun East St, HaiDian District, Beijing, P.R.China, 100190<br />
</p>
</td></tr></table>

<h2>News</h2>
<ul>
<li>
  <p>
      Our paper "Semi-supervised Multimodal Deep Learning for RGB-D Object Recognition" has been accepeted by IJCAI2016.
  </p>
</li>
</ul>

<h2>
  Biography 
</h2>
<ul>

<li>
  <p>
    I am a 4th-year M.D-Ph.D. student in <a href="http://www.nlpr.ia.ac.cn/">National Laboratory of Pattern Recognition (NLPR)</a>, <a href="http://english.ia.cas.cn/">Institute of Automation Chinese Academy of Sciences (CASIA)</a>. My supervisor is Prof. <a href="http://people.ucas.ac.cn/~huangkaiqi">Kaiqi Huang</a>. I am a member of <a href="http://www.cripac.ia.ac.cn/">Center for Research on Intelligent Perception and Computing (CRIPAC)</a>.  
  </p>
</li>

<li>
  <p>
    I received the Bachelor degree in <a href="http://auto.hust.edu.cn/">Department of Automation </a> from <a href="http://www.hust.edu.cn">Huazhong University of Science and Techonology (HUST)</a>. 
  </p>
</li>

<li>
  <p>
    My research interests include computer vision,  deep learning, 3D object/scenet understanding.  
  </p>
</li>
</ul>

<h2>Publications</h2> 
<ul>
<li>
<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">Semi-supervised Learning and Feature Evaluation for RGB-D Object Recognition,</a> <br />
, <b>Yanhua Cheng</b>, Xin Zhao, Kaiqi Huang, Tieniu Tan <br />
<i>Computer Vision and Image Understanding</i> (<b>CVIU</b>), 2015, Volume 139, Pages 149-160. <br />
</li>
[<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">PDF</a>]
[<a href="javascript:toggleBibtex('CVIU15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('CVIU15')" target="_self">Bibtex</a>]
[<a href="http://www.sciencedirect.com/science/article/pii/S1077314215001083">DOI</a>]
<!-- [<a href="https://github.com/cwhgn/EGTracker">Code</a>] -->
<!-- [<a href="http://youtu.be/GZ2u2tvzgi4">Demos</a>] -->
<div class="blockcontent" id="CVIU15_abstract" style="display:none"> 
  <p style="font-size:16px">
        With new depth sensing technology such as Kinect providing high quality synchronized
        RGB and depth images (RGB-D data), combining the two distinct views for
        object recognition has attracted great interest in computer vision and robotics community.
        Recent methods mostly employ supervised learning methods for this new RGB-D
        modality based on the two feature sets. However, supervised learning methods always
        depend on large amount of manually labeled data for training models. To address the
        problem, this paper proposes a semi-supervised learning method to reduce the dependence
        on large annotated training sets. The method can effectively learn from relatively
        plentiful unlabeled data, if powerful feature representations for both the RGB
        and depth view can be extracted. Thus, four representative feature extraction methods
        (KDES, CKM, HMP and CNN-RNN) are analysed and evaluated in this paper, and the
        best is selected for the semi-supervised learning. We verify our method on two publicly
        available RGB-D object databases. The experimental results demonstrate that,
        with only 20% labeled training set, the proposed semi-supervised learning method can
        achieve the state-of-the-art performance on the large-scale RGB-D object database, and
        competitive performance on the 2D3D object database.
      </p>
</div>

<div class="blockcontent" id="CVIU15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/CVIU2015,
 title = {Semi-supervised Learning and Feature Evaluation for RGB-D Object Recognition},
 author={Yanhua Cheng, Xin Zhao, Kaiqi Huang, Tieniu Tan},
 booktitle = {CVIU},
 volume={139}, 
 pages={149--160},
 year = {2015}
}
</pre>
</div>
<br /><br />

<li>
<a href="?"> Semi-supervised Multimodal Deep Learning for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>,Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui <br />
<i>International Joint Conference on Artificial Intelligence </i> (<b>IJCAI</b>), 2015 (accepted). <br />
</li>
[<a href="?">PDF</a>]
[<a href="javascript:toggleBibtex('IJCAI2015_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('IJCAI2015')" target="_self">Bibtex</a>]
[<a href="?">DOI</a>]
<div class="blockcontent" id="IJCAI2015_abstract" style="display:none"> 
  <p style="font-size:16px">
                  This paper studies the problem of RGB-D object
        recognition. Inspired by the great success of deep
        convolutional neural networks (DCNN) in AI, researchers
        have tried to apply it to improve the performance
        of RGB-D object recognition. However,
        DCNN always requires a large-scale annotated
        dataset to supervise its training. Manually labeling
        such a large RGB-D dataset is expensive
        and time consuming, which prevents DCNN from
        quickly promoting this research area. To address
        this problem, we propose a semi-supervised multimodal
        deep learning framework to train DCNN
        effectively based on very limited labeled data and
        massive unlabeled data. The core of our framework
        is a novel diversity preserving co-training algorithm,
        which can successfully guide DCNN to
        learn from the unlabeled RGB-D data by making
        full use of the complementary cues of the RGB and
        depth data in object representation. Experiments
        on the benchmark RGB-D dataset demonstrate that,
        with only 5% labeled training data, our approach
        achieves competitive performance for object recognition
        compared with those state-of-the-art results
        reported by fully-supervised methods.
      </p>
</div>

<div class="blockcontent" id="IJCAI2015" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/IJCAI2015,
 title = {Semi-supervised Multimodal Deep Learning for RGB-D Object Recognition},
 author={Yanhua Cheng, Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui},
 booktitle = {IJCAI},
 year = {2016}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1"> Query Adaptive Similarity Measure for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui <br />
<i>International Conference on Computer Vision</i> (<b>ICCV</b>), 2015. <br />
</li>
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">PDF</a>]
[<a href="javascript:toggleBibtex('ICCV15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICCV15')" target="_self">Bibtex</a>]
[<a href="http://159.226.251.229/videoplayer/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf?ich_u_r_i=0219906766f3baf7140926015524f360&ich_s_t_a_r_t=0&ich_e_n_d=0&ich_k_e_y=1645048907751663462468&ich_t_y_p_e=1&ich_d_i_s_k_i_d=2&ich_u_n_i_t=1">DOI</a>]
<div class="blockcontent" id="ICCV15_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper studies the problem of improving the top-1 accuracy
of RGB-D object recognition. Despite of the impressive
top-5 accuracies achieved by existing methods, their
top-1 accuracies are not very satisfactory. The reasons are
in two-fold: (1) existing similarity measures are sensitive to
object pose and scale changes, as well as intra-class variations;
and (2) effectively fusing RGB and depth cues is still
an open problem. To address these problems, this paper first
proposes a new similarity measure based on dense matching,
through which objects in comparison are warped and
aligned, to better tolerate variations. Towards RGB and
depth fusion, we argue that a constant and golden weight
doesn’t exist. The two modalities have varying contributions
when comparing objects from different categories. To
capture such a dynamic characteristic, a group of matchers
equipped with various fusion weights is constructed, to explore
the responses of dense matching under different fusion
configurations. All the response scores are finally merged
following a learning-to-combination way, which provides
quite good generalization ability in practice. The proposed
approach win the best results on several public benchmarks,
e.g., achieves 92.7% top-1 test accuracy on the Washington
RGB-D object dataset, with a 5.1% improvement over the
state-of-the-art.
      </p>
</div>

<div class="blockcontent" id="ICCV15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/iccv2015,
 title = {Query Adaptive Similarity Measure for RGB-D Object Recognition},
 author={Yanhua Cheng, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui},
 booktitle = {ICCV},
 year = {2015},
 pages = {145-153}
 address = {Chile}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhang_MeshStereo_A_Global_ICCV_2015_paper.html"> MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation, </a> <br />
Chi Zhang, Zhiwei Li, <b>Yanhua Cheng</b>, Rui Cai, Yanghong Chao, Yong Rui<br />
<i>International Conference on Computer Vision</i> (<b>ICCV</b>), 2015 (Oral). <br />
</li>
[<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhang_MeshStereo_A_Global_ICCV_2015_paper.html">PDF</a>]
[<a href="javascript:toggleBibtex('ICCV15-2_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICCV15-2')" target="_self">Bibtex</a>]
[<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhang_MeshStereo_A_Global_ICCV_2015_paper.html">DOI</a>]
<div class="blockcontent" id="ICCV15-2_abstract" style="display:none"> 
  <p style="font-size:16px">
          We present a novel global stereo model designed for view
interpolation. Unlike existing stereo models which only output
a disparity map, our model is able to output a 3D triangular
mesh, which can be directly used for view interpolation.
To realize this feature, we partition the input stereo
images into 2D triangles with shared vertices. Lifting the
2D triangulation to 3D naturally generates a corresponding
mesh. A technical difficulty is to properly split vertices
to multiple copies when they appear at depth discontinuous
boundaries. To deal with this problem, we formulate our objective
as a two-layer MRF, with the upper layer modeling
the splitting properties of the vertices and the lower layer
optimizing a region-based stereo matching. Experiments on
the Middlebury and the Herodion datasets demonstrate that
our model is able to synthesize visually coherent new view
angles with high PSNR, as well as outputting high quality
disparity maps which rank at the first place on the new challenging
high resolution Middlebury 3.0 benchmark.
      </p>
</div>

<div class="blockcontent" id="ICCV15" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/iccv2015,
 title = {MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation},
 author={Chi Zhang, Zhiwei Li, Yanhua Cheng, Rui Cai, Yanghong Chao, Yong Rui},
 booktitle = {ICCV},
 year = {2015},
 pages = {2057-2065}
 address = {Chile}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7335478"> Convolutional Fisher Kernels for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>, Rui Cai, Xin Zhao, Kaiqi Huang <br />
<i>International Conference on 3D Vision </i> (<b>3DV</b>), 2015. <br />
</li>
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7335478">PDF</a>]
[<a href="javascript:toggleBibtex('3DV2015_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('3DV2015')" target="_self">Bibtex</a>]
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7335478">DOI</a>]
<div class="blockcontent" id="3DV2015_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper studies the problem of improving object
recognition using the novel RGB-D data. To address the
problem, a new convolutional Fisher Kernels (CFK) method
is proposed to represent RGB-D objects powerfully yet efficiently.
The core idea of our approach is to integrate the
both advantages of the convolutional neural networks (CNN)
and Fisher Kernel encoding (FK): CNN model is flexible
to adapt to new data sources, but requires for large amounts
of training data with significant computational resources
for good generalization; In comparison, FK encoding is
able to represent objects powerfully and efficiently with small
training data, however, its success highly depends on
the well-designed SIFT features in literature, which may not
be suitable for the new depth data. CFK can be interpreted
as a two-layer feature learning structure to bridge the
two models. The first layer employs a single-layer CNN to
learn low-level translationally invariant features for both
RGB and depth data efficiently. The second layer aggregates
the convolutional responses by FK encoding. Here 2D
and 3D spatial pyramids are applied to further improve the
Fisher vector representation of each modality. Experiments
on RGB-D object recognition benchmarks demonstrate that
our approach can achieve the state-of-the-art results.
      </p>
</div>

<div class="blockcontent" id="3DV2015" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/3DV2015,
 title = {Convolutional Fisher Kernels for RGB-D Object Recognition},
 author={Yanhua Cheng, Rui Cai, Xin Zhao, Kaiqi Huang},
 booktitle = {International Conference on 3D Vision (3DV) 2015},
 year = {2015},
 pages = {135-143}
 address = {France}
}
</pre>
</div>
<br /><br />
</ul>

<li>
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6977124&tag=1"> Semi-Supervised Learning for RGB-D Object Recognition, </a> <br />
<b>Yanhua Cheng</b>, Xin Zhao, Kaiqi Huang, Tieniu Tan <br />
<i>International Conference on Pattern Recognition</i> (<b>ICPR</b>), 2014. <br />
</li>
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6977124&tag=1">PDF</a>]
[<a href="javascript:toggleBibtex('ICPR14_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICPR14')" target="_self">Bibtex</a>]
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6977124&tag=1">DOI</a>]
<div class="blockcontent" id="ICPR14_abstract" style="display:none"> 
  <p style="font-size:16px">
          Abstract—Conventional supervised object recognition methods
have been investigated for many years. Despite their successes,
there are still two suffering limitations: (1) various information
of an object is represented by artificial features only derived
from RGB images; (2) lots of manually labeled data is required
by supervised learning. To address those limitations, we propose
a new semi-supervised learning framework based on RGB and
depth (RGB-D) images to improve object recognition. In particular,
our framework has two modules: (1) RGB and depth images
are represented by convolutional-recursive neural networks to
construct high level features, respectively; (2) co-training is
exploited to make full use of unlabeled RGB-D instances due to
the existing two independent views. Experiments on the standard
RGB-D object dataset demonstrate that our method can compete
against with other state-of-the-art methods with only 20% labeled
data.
      </p>
</div>

<div class="blockcontent" id="ICPR2015" style="display:none"> 
<pre>
@inproceedings{YanhuaCheng/ICPR2015,
 title = {Semi-supervised Learning for RGB-D Object Recognition},
 author={Yanhua Cheng, Xin Zhao, Kaiqi Huang, Tieniu Tan},
 booktitle = {International Conference on Pattern Recognition (ICPR)},
 year = {2015},
 pages = {2377-2382}
 address = {Sweden}
}
</pre>
</div>
<br /><br />
</ul>

<!-- <h2>Softwares</h2>
<ul>
<li>
<a href="http://mct.idealtest.org/Evaluation.html">MCTA Evaluation Toolkit </a> : An evaluation kit for computing Multi-Camera Tracking Accuracy (MCTA) in <a href="http://mct.idealtest.org/">the Multi-Camera Object Tracking (<b>MCT</b>) Challenge </a> in <a href="http://eccv2014.org/">ECCV 2014</a> <a href="http://www.vs-re-id-2014.org/">Workshop on Visual Surveillance and Re-identification</a><br /> 
</li>
</ul> -->

<!-- <h2>Datasets</h2>
<ul>
<li>
  <p>
    <a href="http://mct.idealtest.org/Datasets.html">NLPR_MCT Dataset</a> : The dataset consists of four subsets corresponding to different non-overlapping multi-camera networks.
    </p>
</li>
</ul>

<p>
  <a href="http://english.ia.cas.cn/"><img src="fig/casia.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.nlpr.ia.ac.cn/"><img src="fig/nlpr.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.cripac.ia.ac.cn/"><img src="fig/cripac.png" alt="alt text" width="50px" height="50px"/></a> 

  <a href="http://www.bjtu.edu.cn/"><img src="fig/bjtu.jpg" alt="alt text" width="50px" height="50px"/></a> 
</p>

<div id="footer">
<div id="footer-text">
</br>Last updated at 2015-02-12 by Weihua Chen. Page created by reference to <a href="http://www.ee.cuhk.edu.hk/~rzhao/">Rui Zhao's homepage</a>.
</div>
</div>
</div> -->


</body>
</html>
